{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BoVW.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1FB3gKt0GDrN1nq-ikFMnQEAeqlx5SXw5","authorship_tag":"ABX9TyMRs54uBAmFDpnw5xFSoA8+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"DVE334rXJx0m","colab_type":"code","colab":{}},"source":["! yes | pip3 uninstall opencv-python\n","! yes | pip3 uninstall opencv-contrib-python\n","! yes | pip3 install opencv-python==3.4.2.16\n","! yes | pip3 install opencv-contrib-python==3.4.2.16\n","\n","import cv2\n","import matplotlib.pyplot as plt\n","cv2.__version__\n","\n","sift = cv2.xfeatures2d.SIFT_create()\n","surf = cv2.xfeatures2d.SURF_create()\n","orb = cv2.ORB_create()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UqGoufXEKISh","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","import pandas as pd\n","path = './drive/My Drive/patern/2020bagofwordrcv'\n","codebook_len = 576\n","img_size = 288\n","jump = 6"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tk80SSV9Kgap","colab_type":"code","colab":{}},"source":["## 라벨링 정보를 불러와 사전형으로 저장\n","label_pd = pd.read_csv(path + '/Label2Names.csv',header = None)\n","label_data = np.array(label_pd)\n","\n","label_dict = {}\n","for x in range(len(label_data)):\n","  label_dict[label_data[x][1]] = label_data[x][0]\n","label_dict['BACKGROUND_Google'] = 102"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vqeq3hZDKhHJ","colab_type":"code","colab":{}},"source":["## 이미지를 불러오며 해당 이미지의 라벨과 기술정보를 저장\n","path_train = path + '/train/'\n","#all_train_des = []\n","all_train_label = []\n","a = 0\n","for label in os.listdir(path_train):\n","  print(path_train+label,label_dict[label])\n","  for file in os.listdir(path_train+label):\n","    #img = cv2.imread(path_train+label+'/'+file,cv2.IMREAD_GRAYSCALE)\n","    #img = cv2.resize(img,(img_size,img_size))    \n","    #kp = [cv2.KeyPoint(x,y,jump) for x in range(0,img.shape[1],jump) for y in range(0,img.shape[0],jump)]    \n","    #_,des =sift.compute(img,kp)\n","    #all_train_des.append(des)\n","    all_train_label.append(label_dict[label])\n","  a+=1\n","  print(int((a/len(os.listdir(path_train)))*100+0.5),'%')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9oQoqd_yKjrJ","colab_type":"code","colab":{}},"source":["np.save(path+'/All_des_288x288_6.npy',all_train_des)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S79EWpURKkKR","colab_type":"code","colab":{}},"source":["np.save(path+'/All_label.npy',all_train_label)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tXf8SaBvKq_B","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":298},"outputId":"f19282f4-308b-4faa-e45e-b6a77d833338","executionInfo":{"status":"error","timestamp":1585037831627,"user_tz":-540,"elapsed":1547,"user":{"displayName":"이진수","photoUrl":"","userId":"16195137749659758404"}}},"source":["all_train_des = np.load(path+'/test_des_288x288_6.npy')"],"execution_count":23,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-ad9668b7ec7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_train_des\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/test_des_288x288_6.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './drive/My Drive/patern/2020bagofwordrcv/test_des_288x288_6.npy'"]}]},{"cell_type":"code","metadata":{"id":"J1viXFenKsLR","colab_type":"code","colab":{}},"source":["all_train_label = np.load(path+'/All_label.npy')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_OAbhaTbK0sR","colab_type":"code","colab":{}},"source":["train_des = []\n","train_label = []\n","check_des = []\n","check_label = []\n","for x in range(102):\n","  for y in range(30):\n","    if y < 25 :\n","      train_des.append(all_train_des[x*30+y])\n","      train_label.append(all_train_label[x*30+y])\n","    else :\n","      check_des.append(all_train_des[x*30+y])\n","      check_label.append(all_train_label[x*30+y])\n","print(np.array(train_des).shape,np.array(train_label).shape)\n","print(np.array(check_des).shape,np.array(check_label).shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f4GoxjjqK7Bp","colab_type":"code","colab":{}},"source":["## 학습 이미지의 기술자정보 합침\n","des_list= []\n","for x in range(len(train_des)) :\n","  des_list.extend(train_des[x])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oLNcGd8rK9kA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":343},"outputId":"730f4597-a52a-437d-b6d4-87a472b03988","executionInfo":{"status":"ok","timestamp":1585033397090,"user_tz":-540,"elapsed":16534,"user":{"displayName":"이진수","photoUrl":"","userId":"16195137749659758404"}}},"source":["!pip3 install kmc2\n","import kmc2\n","from sklearn.cluster import MiniBatchKMeans\n","from sklearn.cluster import KMeans"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting kmc2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/a2/42b2dd4fa0c425912c03222dd443f1d6aceed410a29467d1e5d8989c72f1/kmc2-0.1.tar.gz (102kB)\n","\u001b[K     |████████████████████████████████| 102kB 2.4MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from kmc2) (1.18.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from kmc2) (1.4.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from kmc2) (0.22.2.post1)\n","Collecting nose\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n","\u001b[K     |████████████████████████████████| 163kB 7.7MB/s \n","\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->kmc2) (0.14.1)\n","Building wheels for collected packages: kmc2\n","  Building wheel for kmc2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kmc2: filename=kmc2-0.1-cp36-cp36m-linux_x86_64.whl size=252229 sha256=c899af6a7bf447f1485538399513db2c93503e580bc8a0f18db7938d6b09a329\n","  Stored in directory: /root/.cache/pip/wheels/5c/ba/f0/4c8b421be72d4f2d1a93233c2f6f591e7d8b0bda05a1f4616f\n","Successfully built kmc2\n","Installing collected packages: nose, kmc2\n","Successfully installed kmc2-0.1 nose-1.3.7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"43iBCqmAK_AQ","colab_type":"code","colab":{}},"source":["#코드북 생성\n","seeding = kmc2.kmc2(des_list,codebook_len)\n","clustered_mini = MiniBatchKMeans(codebook_len,init=seeding,n_init=1,init_size=int(codebook_len*3.3)).fit(des_list)\n","codebook = clustered_mini.cluster_centers_"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G7EFMACoLIzo","colab_type":"code","colab":{}},"source":["from scipy.cluster.vq import vq"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mgo4-XCmWnkT","colab_type":"code","colab":{}},"source":["## 각각의 이미지의 코드북에대한 히스토그램\n","def SPM (des,codebook,codebook_len,img_size,step):\n","\n","  cut1 = int((img_size/step)*(1/4))\n","  cut2 = int((img_size/step)*(2/4))\n","  cut3 = int((img_size/step)*(3/4))\n","  histogram = []\n","  histograma = []\n","\n","  for x in range(len(des)):\n","    des_map = des[x].reshape(int(img_size/step),int(img_size/step),128)\n","\n","    ## level 0\n","    lv_0,_ = vq(des[x],codebook)\n","    #\n","    lv_0,_ = np.histogram(lv_0, bins=list(range(codebook_len+1)),normed=True)\n","\n","\n","    #\n","    ## level 1\n","    lv_1_1,_ = vq(des_map[:cut2,:cut2,:].reshape(-1,128),codebook)\n","    lv_1_1a,_ = np.histogram(lv_1_1, bins=list(range(codebook_len+1)),normed=True)\n","    lv_1_1a = lv_1_1a.reshape((codebook_len,1))\n","\n","    lv_1_2,_ = vq(des_map[cut2:,:cut2,:].reshape(-1,128),codebook)\n","    lv_1_2a,_ = np.histogram(lv_1_2, bins=list(range(codebook_len+1)),normed=True)\n","    lv_1_2a = lv_1_2a.reshape((codebook_len,1))\n","\n","    lv_1_3,_ = vq(des_map[:cut2,cut2:,:].reshape(-1,128),codebook)\n","    lv_1_3a,_ = np.histogram(lv_1_3, bins=list(range(codebook_len+1)),normed=True)\n","    lv_1_3a = lv_1_3a.reshape((codebook_len,1))\n","\n","    lv_1_4,_ = vq(des_map[cut2:,cut2:,:].reshape(-1,128),codebook)\n","    lv_1_4a,_ = np.histogram(lv_1_4, bins=list(range(codebook_len+1)),normed=True)\n","    lv_1_4a = lv_1_4a.reshape((codebook_len,1))\n","\n","  #\n","    lv_1 = np.concatenate((lv_1_1, lv_1_2, lv_1_3, lv_1_4))\n","    lv_1,_ = np.histogram(lv_1, bins=list(range(codebook_len+1)),normed=True)\n","\n","    lv_1a = np.concatenate((lv_1_1a,lv_1_2a,lv_1_3a,lv_1_4a),axis=1).flatten()\n","\n","    \n","    #\n","    ## level 2\n","    lv_2_1,_ = vq(des_map[:cut1,:cut1,:].reshape(-1,128),codebook)\n","    lv_2_1a,_ = np.histogram(lv_2_1,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_1a = lv_2_1a.reshape((codebook_len,1))\n","    lv_2_2,_ = vq(des_map[cut1:,:cut1,:].reshape(-1,128),codebook)\n","    lv_2_2a,_ = np.histogram(lv_2_2,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_2a = lv_2_2a.reshape((codebook_len,1))\n","    lv_2_3,_ = vq(des_map[cut2:cut3,:cut1,:].reshape(-1,128),codebook)\n","    lv_2_3a,_ = np.histogram(lv_2_3,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_3a = lv_2_3a.reshape((codebook_len,1))\n","    lv_2_4,_ = vq(des_map[cut3:,:cut1,:].reshape(-1,128),codebook)\n","    lv_2_4a,_ = np.histogram(lv_2_4,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_4a = lv_2_4a.reshape((codebook_len,1))\n","    #\n","    lv_2_5,_ = vq(des_map[:cut1,cut1:cut2,:].reshape(-1,128),codebook)\n","    lv_2_5a,_ = np.histogram(lv_2_5,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_5a = lv_2_5a.reshape((codebook_len,1))\n","    lv_2_6,_ = vq(des_map[cut1:,cut1:cut2,:].reshape(-1,128),codebook)\n","    lv_2_6a,_ = np.histogram(lv_2_6,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_6a = lv_2_6a.reshape((codebook_len,1))\n","    lv_2_7,_ = vq(des_map[cut2:cut3,cut1:cut2,:].reshape(-1,128),codebook)\n","    lv_2_7a,_ = np.histogram(lv_2_7,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_7a = lv_2_7a.reshape((codebook_len,1))\n","    lv_2_8,_ = vq(des_map[cut3:,cut1:cut2,:].reshape(-1,128),codebook)\n","    lv_2_8a,_ = np.histogram(lv_2_8,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_8a = lv_2_8a.reshape((codebook_len,1))\n","    #\n","    lv_2_9,_ = vq(des_map[:cut1,cut2:cut3,:].reshape(-1,128),codebook)\n","    lv_2_9a,_ = np.histogram(lv_2_9,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_9a = lv_2_9a.reshape((codebook_len,1))\n","    lv_2_10,_ = vq(des_map[cut1:,cut2:cut3,:].reshape(-1,128),codebook)\n","    lv_2_10a,_ = np.histogram(lv_2_10,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_10a = lv_2_10a.reshape((codebook_len,1))\n","    lv_2_11,_ = vq(des_map[cut2:cut3,cut2:cut3,:].reshape(-1,128),codebook)\n","    lv_2_11a,_ = np.histogram(lv_2_11,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_11a = lv_2_11a.reshape((codebook_len,1))\n","    lv_2_12,_ = vq(des_map[cut3:,cut2:cut3,:].reshape(-1,128),codebook)\n","    lv_2_12a,_ = np.histogram(lv_2_12,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_12a = lv_2_12a.reshape((codebook_len,1))\n","    #\n","    lv_2_13,_ = vq(des_map[:cut1,cut3:,:].reshape(-1,128),codebook)\n","    lv_2_13a,_ = np.histogram(lv_2_13,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_13a = lv_2_13a.reshape((codebook_len,1))\n","    lv_2_14,_ = vq(des_map[cut1:,cut3:,:].reshape(-1,128),codebook)\n","    lv_2_14a,_ = np.histogram(lv_2_14,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_14a = lv_2_14a.reshape((codebook_len,1))\n","    lv_2_15,_ = vq(des_map[cut2:cut3,cut3:,:].reshape(-1,128),codebook)\n","    lv_2_15a,_ = np.histogram(lv_2_15,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_15a = lv_2_15a.reshape((codebook_len,1))\n","    lv_2_16,_ = vq(des_map[cut3:,cut3:,:].reshape(-1,128),codebook)\n","    lv_2_16a,_ = np.histogram(lv_2_16,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_16a = lv_2_16a.reshape((codebook_len,1))\n","    #\n","    lv_2 = np.concatenate((lv_2_1, lv_2_2, lv_2_3, lv_2_4, lv_2_5, lv_2_6,lv_2_7, lv_2_8,\n","                          lv_2_9, lv_2_10, lv_2_11, lv_2_12, lv_2_13, lv_2_14, lv_2_15, lv_2_16))\n","    lv_2,_ = np.histogram(lv_2, bins=list(range(codebook_len+1)),normed=True)\n","\n","    lv_2a = np.concatenate((lv_2_1a, lv_2_2a, lv_2_3a, lv_2_4a, lv_2_5a, lv_2_6a,lv_2_7a, lv_2_8a,\n","                          lv_2_9a, lv_2_10a, lv_2_11a, lv_2_12a, lv_2_13a, lv_2_14a, lv_2_15a, lv_2_16a),axis=1).flatten()\n","    \n","    hist = lv_0*(0.25)+ lv_1*(0.25)+ lv_2*(0.5)\n","    hista = np.concatenate((lv_0*(25), lv_1a*(25), lv_2a*(50)))\n","    histogram.append(hist)\n","    histograma.append(hista)\n","     \n","  return (histograma)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vFb6DLsdLMZR","colab_type":"code","colab":{}},"source":["## 각각의 이미지의 코드북에대한 히스토그램\n","def SPM_2 (des,codebook,codebook_len,img_size,step):\n","\n","  cut1 = int((img_size/step)*(1/4))\n","  cut2 = int((img_size/step)*(2/4))\n","  cut3 = int((img_size/step)*(3/4))\n","\n","  cut4 = int((img_size/step)*(1/3))\n","  cut5 = int((img_size/step)*(2/3))\n","\n","  histogram = []\n","  histograma = []\n","\n","  for x in range(len(des)):\n","    des_map = des[x].reshape(int(img_size/step),int(img_size/step),128)\n","\n","    ## level 0\n","    lv_0,_ = vq(des[x],codebook)\n","    #\n","    lv_0,_ = np.histogram(lv_0, bins=list(range(codebook_len+1)),normed=True)\n","\n","\n","    #\n","    ## level 1\n","    lv_1_1,_ = vq(des_map[:cut2,:cut2,:].reshape(-1,128),codebook)\n","    lv_1_1a,_ = np.histogram(lv_1_1, bins=list(range(codebook_len+1)),normed=True)\n","    lv_1_1a = lv_1_1a.reshape((codebook_len,1))\n","\n","    lv_1_2,_ = vq(des_map[cut2:,:cut2,:].reshape(-1,128),codebook)\n","    lv_1_2a,_ = np.histogram(lv_1_2, bins=list(range(codebook_len+1)),normed=True)\n","    lv_1_2a = lv_1_2a.reshape((codebook_len,1))\n","\n","    lv_1_3,_ = vq(des_map[:cut2,cut2:,:].reshape(-1,128),codebook)\n","    lv_1_3a,_ = np.histogram(lv_1_3, bins=list(range(codebook_len+1)),normed=True)\n","    lv_1_3a = lv_1_3a.reshape((codebook_len,1))\n","\n","    lv_1_4,_ = vq(des_map[cut2:,cut2:,:].reshape(-1,128),codebook)\n","    lv_1_4a,_ = np.histogram(lv_1_4, bins=list(range(codebook_len+1)),normed=True)\n","    lv_1_4a = lv_1_4a.reshape((codebook_len,1))\n","    #\n","    lv_1 = np.concatenate((lv_1_1, lv_1_2, lv_1_3, lv_1_4))\n","    lv_1,_ = np.histogram(lv_1, bins=list(range(codebook_len+1)),normed=True)\n","\n","    lv_1a = np.concatenate((lv_1_1a,lv_1_2a,lv_1_3a,lv_1_4a),axis=1).flatten()\n","\n","    #level 1.5\n","    lv_3_1,_ = vq(des_map[:cut4,:cut4,:].reshape(-1,128),codebook)\n","    lv_3_1a,_ = np.histogram(lv_3_1, bins=list(range(codebook_len+1)),normed=True)\n","    lv_3_1a = lv_3_1a.reshape((codebook_len,1))\n","\n","    lv_3_2,_ = vq(des_map[cut4:cut5,:cut4,:].reshape(-1,128),codebook)\n","    lv_3_2a,_ = np.histogram(lv_3_2, bins=list(range(codebook_len+1)),normed=True)\n","    lv_3_2a = lv_3_2a.reshape((codebook_len,1))\n","\n","    lv_3_3,_ = vq(des_map[cut5:,:cut4,:].reshape(-1,128),codebook)\n","    lv_3_3a,_ = np.histogram(lv_3_3, bins=list(range(codebook_len+1)),normed=True)\n","    lv_3_3a = lv_3_3a.reshape((codebook_len,1))\n","\n","    lv_3_4,_ = vq(des_map[:cut4,cut4:cut5,:].reshape(-1,128),codebook)\n","    lv_3_4a,_ = np.histogram(lv_3_4, bins=list(range(codebook_len+1)),normed=True)\n","    lv_3_4a = lv_3_4a.reshape((codebook_len,1))\n","\n","    lv_3_5,_ = vq(des_map[cut4:cut5,cut4:cut5,:].reshape(-1,128),codebook)\n","    lv_3_5a,_ = np.histogram(lv_3_5, bins=list(range(codebook_len+1)),normed=True)\n","    lv_3_5a = lv_3_5a.reshape((codebook_len,1))\n","\n","    lv_3_6,_ = vq(des_map[cut5:,cut4:cut5,:].reshape(-1,128),codebook)\n","    lv_3_6a,_ = np.histogram(lv_3_6, bins=list(range(codebook_len+1)),normed=True)\n","    lv_3_6a = lv_3_6a.reshape((codebook_len,1))\n","\n","    lv_3_7,_ = vq(des_map[:cut4,cut5:,:].reshape(-1,128),codebook)\n","    lv_3_7a,_ = np.histogram(lv_3_7, bins=list(range(codebook_len+1)),normed=True)\n","    lv_3_7a = lv_3_7a.reshape((codebook_len,1))\n","\n","    lv_3_8,_ = vq(des_map[cut4:cut5,cut5:,:].reshape(-1,128),codebook)\n","    lv_3_8a,_ = np.histogram(lv_3_8, bins=list(range(codebook_len+1)),normed=True)\n","    lv_3_8a = lv_3_8a.reshape((codebook_len,1))\n","\n","    lv_3_9,_ = vq(des_map[cut5:,cut5:,:].reshape(-1,128),codebook)\n","    lv_3_9a,_ = np.histogram(lv_3_9, bins=list(range(codebook_len+1)),normed=True)\n","    lv_3_9a = lv_3_9a.reshape((codebook_len,1))\n","\n","    lv_3 = np.concatenate((lv_3_1, lv_3_2, lv_3_3, lv_3_4, lv_3_5, lv_3_6, lv_3_7, lv_3_8, lv_3_9))\n","    lv_3,_ = np.histogram(lv_3, bins=list(range(codebook_len+1)),normed=True)\n","\n","    lv_3a = np.concatenate((lv_3_1a, lv_3_2a, lv_3_3a, lv_3_4a, lv_3_5a, lv_3_6a,lv_3_7a, lv_3_8a, lv_3_9a),axis=1).flatten()\n","\n","    \n","    #\n","    ## level 2\n","    lv_2_1,_ = vq(des_map[:cut1,:cut1,:].reshape(-1,128),codebook)\n","    lv_2_1a,_ = np.histogram(lv_2_1,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_1a = lv_2_1a.reshape((codebook_len,1))\n","    lv_2_2,_ = vq(des_map[cut1:,:cut1,:].reshape(-1,128),codebook)\n","    lv_2_2a,_ = np.histogram(lv_2_2,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_2a = lv_2_2a.reshape((codebook_len,1))\n","    lv_2_3,_ = vq(des_map[cut2:cut3,:cut1,:].reshape(-1,128),codebook)\n","    lv_2_3a,_ = np.histogram(lv_2_3,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_3a = lv_2_3a.reshape((codebook_len,1))\n","    lv_2_4,_ = vq(des_map[cut3:,:cut1,:].reshape(-1,128),codebook)\n","    lv_2_4a,_ = np.histogram(lv_2_4,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_4a = lv_2_4a.reshape((codebook_len,1))\n","    #\n","    lv_2_5,_ = vq(des_map[:cut1,cut1:cut2,:].reshape(-1,128),codebook)\n","    lv_2_5a,_ = np.histogram(lv_2_5,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_5a = lv_2_5a.reshape((codebook_len,1))\n","    lv_2_6,_ = vq(des_map[cut1:,cut1:cut2,:].reshape(-1,128),codebook)\n","    lv_2_6a,_ = np.histogram(lv_2_6,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_6a = lv_2_6a.reshape((codebook_len,1))\n","    lv_2_7,_ = vq(des_map[cut2:cut3,cut1:cut2,:].reshape(-1,128),codebook)\n","    lv_2_7a,_ = np.histogram(lv_2_7,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_7a = lv_2_7a.reshape((codebook_len,1))\n","    lv_2_8,_ = vq(des_map[cut3:,cut1:cut2,:].reshape(-1,128),codebook)\n","    lv_2_8a,_ = np.histogram(lv_2_8,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_8a = lv_2_8a.reshape((codebook_len,1))\n","    #\n","    lv_2_9,_ = vq(des_map[:cut1,cut2:cut3,:].reshape(-1,128),codebook)\n","    lv_2_9a,_ = np.histogram(lv_2_9,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_9a = lv_2_9a.reshape((codebook_len,1))\n","    lv_2_10,_ = vq(des_map[cut1:,cut2:cut3,:].reshape(-1,128),codebook)\n","    lv_2_10a,_ = np.histogram(lv_2_10,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_10a = lv_2_10a.reshape((codebook_len,1))\n","    lv_2_11,_ = vq(des_map[cut2:cut3,cut2:cut3,:].reshape(-1,128),codebook)\n","    lv_2_11a,_ = np.histogram(lv_2_11,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_11a = lv_2_11a.reshape((codebook_len,1))\n","    lv_2_12,_ = vq(des_map[cut3:,cut2:cut3,:].reshape(-1,128),codebook)\n","    lv_2_12a,_ = np.histogram(lv_2_12,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_12a = lv_2_12a.reshape((codebook_len,1))\n","    #\n","    lv_2_13,_ = vq(des_map[:cut1,cut3:,:].reshape(-1,128),codebook)\n","    lv_2_13a,_ = np.histogram(lv_2_13,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_13a = lv_2_13a.reshape((codebook_len,1))\n","    lv_2_14,_ = vq(des_map[cut1:,cut3:,:].reshape(-1,128),codebook)\n","    lv_2_14a,_ = np.histogram(lv_2_14,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_14a = lv_2_14a.reshape((codebook_len,1))\n","    lv_2_15,_ = vq(des_map[cut2:cut3,cut3:,:].reshape(-1,128),codebook)\n","    lv_2_15a,_ = np.histogram(lv_2_15,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_15a = lv_2_15a.reshape((codebook_len,1))\n","    lv_2_16,_ = vq(des_map[cut3:,cut3:,:].reshape(-1,128),codebook)\n","    lv_2_16a,_ = np.histogram(lv_2_16,bins=list(range(codebook_len+1)),normed=True)\n","    lv_2_16a = lv_2_16a.reshape((codebook_len,1))\n","    #\n","    lv_2 = np.concatenate((lv_2_1, lv_2_2, lv_2_3, lv_2_4, lv_2_5, lv_2_6,lv_2_7, lv_2_8,\n","                          lv_2_9, lv_2_10, lv_2_11, lv_2_12, lv_2_13, lv_2_14, lv_2_15, lv_2_16))\n","    lv_2,_ = np.histogram(lv_2, bins=list(range(codebook_len+1)),normed=True)\n","\n","    lv_2a = np.concatenate((lv_2_1a, lv_2_2a, lv_2_3a, lv_2_4a, lv_2_5a, lv_2_6a,lv_2_7a, lv_2_8a,\n","                          lv_2_9a, lv_2_10a, lv_2_11a, lv_2_12a, lv_2_13a, lv_2_14a, lv_2_15a, lv_2_16a),axis=1).flatten()\n","    \n","\n","\n","\n","    hist = lv_0*(0.25)+ lv_1*(0.25)+ lv_2*(0.5)\n","    hista = np.concatenate((lv_0*(15), lv_1a*(20), lv_2a*(40), lv_3a*(25)))\n","    histogram.append(hist)\n","    histograma.append(hista)\n","     \n","  return (histograma)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jpIavhHjLbu6","colab_type":"code","colab":{}},"source":["def HistogramIntersection(M, N):\n","    m = np.array(M).shape[0]\n","    n = np.array(N).shape[0]\n","    result = np.zeros((m,n))\n","    for i in range(m):\n","        for j in range(n):\n","            temp = np.sum(np.minimum(M[i], N[j]))\n","            result[i][j] = temp\n","    return result"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qEzxAZSSLNbS","colab_type":"code","colab":{}},"source":["## 학습 데이터 가공 SPM, HistotramIntersection\n","train_histogram = SPM(train_des,codebook,codebook_len,img_size,jump)\n","train_histogram = HistogramIntersection(train_histogram,train_histogram)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MqcF2DmvLcOi","colab_type":"code","colab":{}},"source":["from sklearn import svm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"90NGxSzUL-KT","colab_type":"code","colab":{}},"source":["## 모델 학습\n","model = svm.SVC(kernel='precomputed').fit(train_histogram, train_label)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3M487JzLX4q","colab_type":"code","colab":{}},"source":["## 확인 데이터 가공\n","check_histogram = SPM(check_des,codebook,codebook_len,img_size,jump)\n","check_histogram = HistogramIntersection(histogram_check,train_histogram)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F1_kK-CSMDpB","colab_type":"code","colab":{}},"source":["## 결과 예측\n","predict_check = model.predict(check_histogram)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TWP7uSKEMVP6","colab_type":"code","colab":{}},"source":["## 예측 점수 출력\n","score = 0\n","for x in range(len(predict_check)):\n","  if (predict_check[x] == check_label[x]):\n","    score = score + 1  \n","print('%.2f%%'%((score/len(predict_check))*100))\n","\n","#no spm 33.73\n","#spm 42.94 40.39\n","#HI 256x256_8 53.33%\n","#HI 288x288_6 52.55%"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hIUgamIDNkED","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZoG8UBGZN4x7","colab_type":"code","colab":{}},"source":["## 모든 이미지의 기술정보 합침\n","all_des_list= []\n","for x in range(len(all_train_des)) :\n","  all_des_list.extend(all_train_des[x])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m07Q6BlWODHj","colab_type":"code","colab":{}},"source":["#코드북 생성\n","all_seeding = kmc2.kmc2(all_des_list,codebook_len)\n","all_clustered_mini = MiniBatchKMeans(codebook_len,init=all_seeding,n_init=1,init_size=int(codebook_len*3.3)).fit(all_des_list)\n","all_codebook = all_clustered_mini.cluster_centers_"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zvgTSsrIplq-","colab_type":"code","colab":{}},"source":["np.save(path+'/all_codebook_576_.npy',all_codebook)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UmpC6MtDTRSF","colab_type":"code","colab":{}},"source":["all_codebook = all_clustered_mini.cluster_centers_"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f_ctcgSOMZpy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":469},"outputId":"8b7c505b-e079-4a8f-e489-01532746e482","executionInfo":{"status":"ok","timestamp":1585036624439,"user_tz":-540,"elapsed":2418232,"user":{"displayName":"이진수","photoUrl":"","userId":"16195137749659758404"}}},"source":["## 모든 이미지로 학습\n","## 학습 데이터 가공\n","all_train_histogram = SPM(all_train_des, all_codebook, codebook_len, img_size, jump)\n","all_train_histogram_HI = HistogramIntersection(all_train_histogram, all_train_histogram)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","  from ipykernel import kernelapp as app\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:65: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:68: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:72: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:75: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:78: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:81: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:85: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:88: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:94: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"KCk9oOo1zB5i","colab_type":"code","colab":{}},"source":["all_train_histogram = SPM(all_train_des, all_codebook, codebook_len, img_size, jump)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HITrzZ2pMrH6","colab_type":"code","colab":{}},"source":["model_all = svm.SVC(kernel=\"precomputed\").fit(all_train_histogram_HI, all_train_label)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eYgfg-hKMyOr","colab_type":"code","colab":{}},"source":["# 테스트 이미지를 불러오며 각 이미지의 이름과 기술정보를 저장\n","path_test = path + '/test/'\n","#test_des = []\n","test_file_name = []\n","a = 0\n","for file in os.listdir(path_test):\n","  #img = cv2.imread(path_test+file,cv2.IMREAD_GRAYSCALE)\n","  #img = cv2.resize(img,(img_size,img_size))\n","  #kp = [cv2.KeyPoint(x,y,jump) for x in range(0,img.shape[1],jump) for y in range(0,img.shape[0],jump)] \n","  #_,des =sift.compute(img,kp)\n","  #test_des.append(des)\n","  test_file_name.append(file)\n","  a+=1\n","  if(a%10==0):\n","    print(a)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g4seBnmcM1L7","colab_type":"code","colab":{}},"source":["np.save(path+'/test_des_%dx%d_%d.npy'%(img_size,img_size,jump),test_des)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZVOT4E1INNci","colab_type":"code","colab":{}},"source":["np.save(path+'/test_file_name.npy',test_fime_name)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bpsGUdEANTPj","colab_type":"code","colab":{}},"source":["test_des = np.load(path+'/test_des_256x256_8.npy')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i234OcpENUdj","colab_type":"code","colab":{}},"source":["test_file_name = np.load(path+'/test_file_name.npy')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zMgKZQYGNa7L","colab_type":"code","colab":{}},"source":["test_histogram = SPM(test_des,codebook_all,codebook_len,img_size,jump)\n","test_histogram_HI = histogramIntersection(test_histogram,all_train_histogram)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mSf4QDytzgGS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":172},"outputId":"a8058302-9a0f-446b-99b2-8c818666d399","executionInfo":{"status":"error","timestamp":1585046563621,"user_tz":-540,"elapsed":1016,"user":{"displayName":"이진수","photoUrl":"","userId":"16195137749659758404"}}},"source":["all_predict = model_all.predict(test_histogram_HI)"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-1eb9744e9ffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_histogram_HI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model_all' is not defined"]}]},{"cell_type":"code","metadata":{"id":"b9c3OcoezuM5","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}